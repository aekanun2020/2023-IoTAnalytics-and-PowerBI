{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e5c53d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## โค้ดนี้พัฒนาจาก AC-aggwithWindow-withOrderedWindow.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "543171f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary to change the Kafka broker's IP number to the INTERNAL IP Address and port 9092\n",
    "\n",
    "kafka_broker = \"10.128.0.12:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4287322d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "com.microsoft.azure#spark-mssql-connector added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-75564d47-8b6a-486a-bc93-da8d0173b078;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound com.microsoft.azure#spark-mssql-connector;1.0.2 in central\n",
      "\tfound com.microsoft.sqlserver#mssql-jdbc;8.4.1.jre8 in central\n",
      ":: resolution report :: resolve 708ms :: artifacts dl 16ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\tcom.microsoft.azure#spark-mssql-connector;1.0.2 from central in [default]\n",
      "\tcom.microsoft.sqlserver#mssql-jdbc;8.4.1.jre8 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-75564d47-8b6a-486a-bc93-da8d0173b078\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/13ms)\n",
      "23/06/22 16:18:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Configuring the settings for the Spark job\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"KafkaSubscribe\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"1000m\").\\\n",
    "        config(\"spark.executor.cores\", \"2\").\\\n",
    "        config(\"spark.cores.max\", \"6\").\\\n",
    "        config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0,com.microsoft.azure:spark-mssql-connector:1.0.2\").\\\n",
    "        getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a0fd987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe from event stream coming from a Kafka topic\n",
    "\n",
    "rawMetadata_df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n",
    "  .option(\"subscribe\", \"quickstart-events\") \\\n",
    "  .option(\"group.id\", \"Aekanun-Spark-App\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e1c181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Spark functions\n",
    "\n",
    "from pyspark.sql import functions as sparkf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42b164c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the schema of the raw data\n",
    "\n",
    "rawMetadata_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "904e89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only key and value columns and converting their data type to string\n",
    "\n",
    "onlyMetadata_df = rawMetadata_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59410363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a schema for the incoming JSON data\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"locationId\", StringType()),\n",
    "    StructField(\"location\", StringType()),\n",
    "    StructField(\"parameter\", StringType()),\n",
    "    StructField(\"value\", StringType()),\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"unit\", StringType()),\n",
    "    StructField(\"coordinates\", StringType()),\n",
    "    StructField(\"country\", StringType()),\n",
    "    StructField(\"city\", StringType()),\n",
    "    StructField(\"isMobile\", StringType()),\n",
    "    StructField(\"isAnalysis\", StringType()),\n",
    "    StructField(\"entity\", StringType()),\n",
    "    StructField(\"sensorType\", StringType())\n",
    "])\n",
    "\n",
    "\n",
    "# Nested 'date' and 'coordinates' schema\n",
    "\n",
    "date_schema = StructType([\n",
    "    StructField(\"utc\", StringType()),\n",
    "    StructField(\"local\", StringType())\n",
    "])\n",
    "\n",
    "coordinates_schema = StructType([\n",
    "    StructField(\"latitude\", DoubleType()),\n",
    "    StructField(\"longitude\", DoubleType())\n",
    "])\n",
    "\n",
    "\n",
    "# Parsing the JSON string column and converting it to a struct\n",
    "\n",
    "parsedData_df = onlyMetadata_df.withColumn(\"data\", from_json(\"value\", schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebbe04a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- locationId: string (nullable = true)\n",
      " |    |-- location: string (nullable = true)\n",
      " |    |-- parameter: string (nullable = true)\n",
      " |    |-- value: string (nullable = true)\n",
      " |    |-- date: string (nullable = true)\n",
      " |    |-- unit: string (nullable = true)\n",
      " |    |-- coordinates: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- isMobile: string (nullable = true)\n",
      " |    |-- isAnalysis: string (nullable = true)\n",
      " |    |-- entity: string (nullable = true)\n",
      " |    |-- sensorType: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsedData_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d6e9cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only the nested fields\n",
    "\n",
    "unNested_df = parsedData_df.select(\"key\", \"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46a6d03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- locationId: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- parameter: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- unit: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- isMobile: string (nullable = true)\n",
      " |-- isAnalysis: string (nullable = true)\n",
      " |-- entity: string (nullable = true)\n",
      " |-- sensorType: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unNested_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab553d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- locationId: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- parameter: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- date: struct (nullable = true)\n",
      " |    |-- utc: string (nullable = true)\n",
      " |    |-- local: string (nullable = true)\n",
      " |-- unit: string (nullable = true)\n",
      " |-- coordinates: struct (nullable = true)\n",
      " |    |-- latitude: double (nullable = true)\n",
      " |    |-- longitude: double (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- isMobile: string (nullable = true)\n",
      " |-- isAnalysis: string (nullable = true)\n",
      " |-- entity: string (nullable = true)\n",
      " |-- sensorType: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- locationId: string (nullable = true)\n",
      " |    |-- location: string (nullable = true)\n",
      " |    |-- parameter: string (nullable = true)\n",
      " |    |-- value: string (nullable = true)\n",
      " |    |-- date: string (nullable = true)\n",
      " |    |-- unit: string (nullable = true)\n",
      " |    |-- coordinates: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- isMobile: string (nullable = true)\n",
      " |    |-- isAnalysis: string (nullable = true)\n",
      " |    |-- entity: string (nullable = true)\n",
      " |    |-- sensorType: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing for Extraction of 'date' and 'coordinates' fields from the nested structure\n",
    "\n",
    "unNested_df.withColumn(\"data\", from_json(\"value\", schema))\\\n",
    ".withColumn(\"date\", from_json(\"date\", date_schema))\\\n",
    ".withColumn(\"coordinates\", from_json(\"coordinates\", coordinates_schema))\\\n",
    ".printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "200a8694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting 'date' and 'coordinates' fields from the nested structure\n",
    "\n",
    "extractedDateLatLong_df = unNested_df.withColumn(\"data\", from_json(\"value\", schema))\\\n",
    ".withColumn(\"date\", from_json(\"date\", date_schema))\\\n",
    ".withColumn(\"coordinates\", from_json(\"coordinates\", coordinates_schema))\\\n",
    ".select('key',\n",
    " 'locationId',\n",
    " 'location',\n",
    " 'parameter',\n",
    " 'value',\n",
    " 'date.*',\n",
    " 'unit',\n",
    " 'coordinates.*',\n",
    " 'country',\n",
    " 'city',\n",
    " 'isMobile',\n",
    " 'isAnalysis',\n",
    " 'entity',\n",
    " 'sensorType')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58828070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- locationId: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- parameter: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- utc: string (nullable = true)\n",
      " |-- local: string (nullable = true)\n",
      " |-- unit: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- isMobile: string (nullable = true)\n",
      " |-- isAnalysis: string (nullable = true)\n",
      " |-- entity: string (nullable = true)\n",
      " |-- sensorType: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extractedDateLatLong_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1926395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'utc' column to timestamp and 'value' column to float\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "final_df = extractedDateLatLong_df\\\n",
    ".withColumn('sourceSTP',sparkf.to_timestamp(sparkf.col('utc'), \"yyyy-MM-dd'T'HH:mm:ssXXX\"))\\\n",
    ".withColumn('value',sparkf.col('value').cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cb53b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['key',\n",
       " 'locationId',\n",
       " 'location',\n",
       " 'parameter',\n",
       " 'value',\n",
       " 'utc',\n",
       " 'local',\n",
       " 'unit',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'country',\n",
       " 'city',\n",
       " 'isMobile',\n",
       " 'isAnalysis',\n",
       " 'entity',\n",
       " 'sensorType',\n",
       " 'sourceSTP']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb975d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- locationId: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- parameter: string (nullable = true)\n",
      " |-- value: float (nullable = true)\n",
      " |-- utc: string (nullable = true)\n",
      " |-- local: string (nullable = true)\n",
      " |-- unit: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- isMobile: string (nullable = true)\n",
      " |-- isAnalysis: string (nullable = true)\n",
      " |-- entity: string (nullable = true)\n",
      " |-- sensorType: string (nullable = true)\n",
      " |-- sourceSTP: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db0c0ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/22 16:18:32 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-be2f5120-25d7-4f72-a073-6186c81210cf. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import window\n",
    "\n",
    "windowedCounts = final_df.groupBy(\n",
    "    window(final_df.sourceSTP, \"360 minutes\", \"360 minutes\"),\n",
    "    final_df.value\n",
    ").count()\n",
    "\n",
    "orderwindowedCounts = windowedCounts.orderBy('window','value')\n",
    "\n",
    "query = orderwindowedCounts.writeStream.queryName(\"aggWindow\").outputMode(\"complete\").format(\"memory\").start()\n",
    "query.awaitTermination(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5a16a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-----+-----+\n",
      "|window                                    |value|count|\n",
      "+------------------------------------------+-----+-----+\n",
      "|[2022-12-31 18:00:00, 2023-01-01 00:00:00]|36.0 |6    |\n",
      "|[2023-01-01 00:00:00, 2023-01-01 06:00:00]|30.0 |2    |\n",
      "|[2023-01-01 00:00:00, 2023-01-01 06:00:00]|31.0 |2    |\n",
      "|[2023-01-01 00:00:00, 2023-01-01 06:00:00]|33.0 |1    |\n",
      "|[2023-01-01 00:00:00, 2023-01-01 06:00:00]|34.0 |1    |\n",
      "|[2023-01-01 06:00:00, 2023-01-01 12:00:00]|29.0 |6    |\n",
      "|[2023-01-01 12:00:00, 2023-01-01 18:00:00]|29.0 |2    |\n",
      "|[2023-01-01 12:00:00, 2023-01-01 18:00:00]|30.0 |4    |\n",
      "|[2023-01-01 18:00:00, 2023-01-02 00:00:00]|31.0 |4    |\n",
      "|[2023-01-01 18:00:00, 2023-01-02 00:00:00]|32.0 |2    |\n",
      "|[2023-01-02 00:00:00, 2023-01-02 06:00:00]|33.0 |2    |\n",
      "|[2023-01-02 00:00:00, 2023-01-02 06:00:00]|34.0 |1    |\n",
      "|[2023-01-02 00:00:00, 2023-01-02 06:00:00]|35.0 |2    |\n",
      "|[2023-01-02 00:00:00, 2023-01-02 06:00:00]|36.0 |1    |\n",
      "|[2023-01-02 06:00:00, 2023-01-02 12:00:00]|36.0 |1    |\n",
      "|[2023-01-02 06:00:00, 2023-01-02 12:00:00]|37.0 |5    |\n",
      "|[2023-01-02 12:00:00, 2023-01-02 18:00:00]|37.0 |3    |\n",
      "|[2023-01-02 12:00:00, 2023-01-02 18:00:00]|38.0 |3    |\n",
      "|[2023-01-02 18:00:00, 2023-01-03 00:00:00]|38.0 |5    |\n",
      "|[2023-01-03 06:00:00, 2023-01-03 12:00:00]|36.0 |1    |\n",
      "+------------------------------------------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from aggWindow').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a76cc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_unixtime, unix_timestamp\n",
    "\n",
    "new_df = spark.sql('select * from aggWindow') \\\n",
    "    .withColumn(\"start_window\", from_unixtime(unix_timestamp(col(\"window.start\")))) \\\n",
    "    .withColumn(\"end_window\", from_unixtime(unix_timestamp(col(\"window.end\")))) \\\n",
    "    .drop(\"window\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8df22cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4c9dfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('select * from aggWindow').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f039c3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: float (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      " |-- start_window: string (nullable = true)\n",
      " |-- end_window: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eaa68e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------------+-------------------+\n",
      "|value|count|       start_window|         end_window|\n",
      "+-----+-----+-------------------+-------------------+\n",
      "| 36.0|    6|2022-12-31 18:00:00|2023-01-01 00:00:00|\n",
      "| 30.0|    2|2023-01-01 00:00:00|2023-01-01 06:00:00|\n",
      "| 31.0|    2|2023-01-01 00:00:00|2023-01-01 06:00:00|\n",
      "| 33.0|    1|2023-01-01 00:00:00|2023-01-01 06:00:00|\n",
      "| 34.0|    1|2023-01-01 00:00:00|2023-01-01 06:00:00|\n",
      "| 29.0|    6|2023-01-01 06:00:00|2023-01-01 12:00:00|\n",
      "| 29.0|    2|2023-01-01 12:00:00|2023-01-01 18:00:00|\n",
      "| 30.0|    4|2023-01-01 12:00:00|2023-01-01 18:00:00|\n",
      "| 31.0|    4|2023-01-01 18:00:00|2023-01-02 00:00:00|\n",
      "| 32.0|    2|2023-01-01 18:00:00|2023-01-02 00:00:00|\n",
      "| 33.0|    2|2023-01-02 00:00:00|2023-01-02 06:00:00|\n",
      "| 34.0|    1|2023-01-02 00:00:00|2023-01-02 06:00:00|\n",
      "| 35.0|    2|2023-01-02 00:00:00|2023-01-02 06:00:00|\n",
      "| 36.0|    1|2023-01-02 00:00:00|2023-01-02 06:00:00|\n",
      "| 36.0|    1|2023-01-02 06:00:00|2023-01-02 12:00:00|\n",
      "| 37.0|    5|2023-01-02 06:00:00|2023-01-02 12:00:00|\n",
      "| 37.0|    3|2023-01-02 12:00:00|2023-01-02 18:00:00|\n",
      "| 38.0|    3|2023-01-02 12:00:00|2023-01-02 18:00:00|\n",
      "| 38.0|    5|2023-01-02 18:00:00|2023-01-03 00:00:00|\n",
      "| 36.0|    1|2023-01-03 06:00:00|2023-01-03 12:00:00|\n",
      "+-----+-----+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db336e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: float (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      " |-- start_window: timestamp (nullable = true)\n",
      " |-- end_window: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "timestamped_df = new_df.withColumn(\"start_window\", to_timestamp(col(\"start_window\"))) \\\n",
    "    .withColumn(\"end_window\", to_timestamp(col(\"end_window\")))\n",
    "\n",
    "timestamped_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17fbbd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------------+-------------------+\n",
      "|value|count|       start_window|         end_window|\n",
      "+-----+-----+-------------------+-------------------+\n",
      "| 36.0|    6|2022-12-31 18:00:00|2023-01-01 00:00:00|\n",
      "| 30.0|    2|2023-01-01 00:00:00|2023-01-01 06:00:00|\n",
      "| 31.0|    2|2023-01-01 00:00:00|2023-01-01 06:00:00|\n",
      "| 33.0|    1|2023-01-01 00:00:00|2023-01-01 06:00:00|\n",
      "| 34.0|    1|2023-01-01 00:00:00|2023-01-01 06:00:00|\n",
      "| 29.0|    6|2023-01-01 06:00:00|2023-01-01 12:00:00|\n",
      "| 29.0|    2|2023-01-01 12:00:00|2023-01-01 18:00:00|\n",
      "| 30.0|    4|2023-01-01 12:00:00|2023-01-01 18:00:00|\n",
      "| 31.0|    4|2023-01-01 18:00:00|2023-01-02 00:00:00|\n",
      "| 32.0|    2|2023-01-01 18:00:00|2023-01-02 00:00:00|\n",
      "| 33.0|    2|2023-01-02 00:00:00|2023-01-02 06:00:00|\n",
      "| 34.0|    1|2023-01-02 00:00:00|2023-01-02 06:00:00|\n",
      "| 35.0|    2|2023-01-02 00:00:00|2023-01-02 06:00:00|\n",
      "| 36.0|    1|2023-01-02 00:00:00|2023-01-02 06:00:00|\n",
      "| 36.0|    1|2023-01-02 06:00:00|2023-01-02 12:00:00|\n",
      "| 37.0|    5|2023-01-02 06:00:00|2023-01-02 12:00:00|\n",
      "| 37.0|    3|2023-01-02 12:00:00|2023-01-02 18:00:00|\n",
      "| 38.0|    3|2023-01-02 12:00:00|2023-01-02 18:00:00|\n",
      "| 38.0|    5|2023-01-02 18:00:00|2023-01-03 00:00:00|\n",
      "| 36.0|    1|2023-01-03 06:00:00|2023-01-03 12:00:00|\n",
      "+-----+-----+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timestamped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4259a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamped_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88e0e68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e2cd677",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "server_name = \"jdbc:sqlserver://10.128.0.66\"\n",
    "database_name = \"testDB\"\n",
    "url = server_name + \";\" + \"databaseName=\" + database_name + \";\"\n",
    "\n",
    "table_name = \"aekanun_pm2022\"\n",
    "username = \"SA\"\n",
    "password = \"Passw0rd123456\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ade7d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set the connection properties\n",
    "properties = {\n",
    "    \"user\": username,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "\n",
    "# Assuming 'read_result_df' is your DataFrame\n",
    "timestamped_df.write.jdbc(url=url, table=table_name , mode='overwrite', properties=properties)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e738769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30331059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a82b91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
