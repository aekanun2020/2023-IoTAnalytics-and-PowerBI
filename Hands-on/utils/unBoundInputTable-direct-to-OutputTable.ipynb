{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96a9b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## โค้ดนี้พัฒนาจาก A-aggValuePM3-alertDisplay-withTrigger.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed676334",
   "metadata": {},
   "outputs": [],
   "source": [
    "## โจทย์ทางธุรกิจ คือ ธุรกิจต้องการรับการแจ้งเตือนเมื่อมีค่าฝุ่น (pm 2.5) เกินกว่าค่า threshold \n",
    "## โดยให้มีการคำนวณเปรียบเทียบกับ threshold ในทุกๆ 5 วินาทีที่ processing platform (Spark) ได้รับข้อมูล (Message Arrival)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "699f914b",
   "metadata": {},
   "source": [
    "ข้อมูลที่ publish เข้าสู่ Kafka broker คือ 2022-01-01-2022-12-31.csv ซึ่งมีหน้าตาแบบนี้\n",
    "\n",
    "{\"locationId\": \"225577\", \"location\": \"Bank of Ayuthaya Head Office Yan Nawa, Bangkok\", \"parameter\": \"pm25\", \"value\": \"35.0\", \"date\": \"{'utc': '2022-12-31T04:00:00+00:00', 'local': '2022-12-31T11:00:00+07:00'}\", \"unit\": \"\\u00b5g/m\\u00b3\", \"coordinates\": \"{'latitude': 13.679226, 'longitude': 100.54687}\", \"country\": \"NA\", \"city\": \"\", \"isMobile\": \"False\", \"isAnalysis\": \"\", \"entity\": \"Governmental Organization\", \"sensorType\": \"reference grade\"}\n",
    "{\"locationId\": \"225577\", \"location\": \"Bank of Ayuthaya Head Office Yan Nawa, Bangkok\", \"parameter\": \"pm25\", \"value\": \"36.0\", \"date\": \"{'utc': '2022-12-31T05:00:00+00:00', 'local': '2022-12-31T12:00:00+07:00'}\", \"unit\": \"\\u00b5g/m\\u00b3\", \"coordinates\": \"{'latitude': 13.679226, 'longitude': 100.54687}\", \"country\": \"NA\", \"city\": \"\", \"isMobile\": \"False\", \"isAnalysis\": \"\", \"entity\": \"Governmental Organization\", \"sensorType\": \"reference grade\"}\n",
    "{\"locationId\": \"225577\", \"location\": \"Bank of Ayuthaya Head Office Yan Nawa, Bangkok\", \"parameter\": \"pm25\", \"value\": \"36.0\", \"date\": \"{'utc': '2022-12-31T06:00:00+00:00', 'local': '2022-12-31T13:00:00+07:00'}\", \"unit\": \"\\u00b5g/m\\u00b3\", \"coordinates\": \"{'latitude': 13.679226, 'longitude': 100.54687}\", \"country\": \"NA\", \"city\": \"\", \"isMobile\": \"False\", \"isAnalysis\": \"\", \"entity\": \"Governmental Organization\", \"sensorType\": \"reference grade\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bd1fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# จำเป็นต้องเปลี่ยนเลขไอพีของ Kafka broker โดยใช้ INTERNAL IP Address พร้อมด้วยหมายเลข port 9092\n",
    "\n",
    "kafka_broker = \"10.128.0.12:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9caf5c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b8df3ea7-5f68-4a32-bacf-f1c34d302b6f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 652ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b8df3ea7-5f68-4a32-bacf-f1c34d302b6f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/12ms)\n",
      "23/06/22 00:27:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# สร้าง Spark session โดยใช้การตั้งค่าที่กำหนด\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"KafkaSubscribe\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"1000m\").\\\n",
    "        config(\"spark.executor.cores\", \"2\").\\\n",
    "        config(\"spark.cores.max\", \"6\").\\\n",
    "        config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0\").\\\n",
    "        getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f009b77",
   "metadata": {},
   "source": [
    "# 1. ได้รับ Unbounded input table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e51d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# สร้าง DataFrame โดยอ่านข้อมูลจาก Kafka stream\n",
    "\n",
    "unboundInput_table_df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n",
    "  .option(\"subscribe\", \"quickstart-events\") \\\n",
    "  .option(\"group.id\", \"Aekanun-Spark-App\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78aad959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sparkf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b98fa759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# แสดง schema ของ column ต่างๆ ใน DataFrame\n",
    "\n",
    "unboundInput_table_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea57b85",
   "metadata": {},
   "source": [
    "# 2. เกิดการ Query - Stream Data Processing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30eb2085",
   "metadata": {},
   "source": [
    "# เลือกคอลัมน์ที่ต้องการจาก DataFrame\n",
    "\n",
    "selectedCol_df = unboundInput_table_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7d3b564",
   "metadata": {},
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "# กำหนด schema สำหรับข้อมูลที่ต้องการ\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"locationId\", StringType()),\n",
    "    StructField(\"location\", StringType()),\n",
    "    StructField(\"parameter\", StringType()),\n",
    "    StructField(\"value\", StringType()),\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"unit\", StringType()),\n",
    "    StructField(\"coordinates\", StringType()),\n",
    "    StructField(\"country\", StringType()),\n",
    "    StructField(\"city\", StringType()),\n",
    "    StructField(\"isMobile\", StringType()),\n",
    "    StructField(\"isAnalysis\", StringType()),\n",
    "    StructField(\"entity\", StringType()),\n",
    "    StructField(\"sensorType\", StringType())\n",
    "])\n",
    "\n",
    "date_schema = StructType([\n",
    "    StructField(\"utc\", StringType()),\n",
    "    StructField(\"local\", StringType())\n",
    "])\n",
    "\n",
    "coordinates_schema = StructType([\n",
    "    StructField(\"latitude\", DoubleType()),\n",
    "    StructField(\"longitude\", DoubleType())\n",
    "])\n",
    "\n",
    "# แปลงข้อมูลที่มีโครงสร้างเป็น JSON ใน column value เป็น schema ที่กำหนดไว้ก่อนหน้านี้\n",
    "parsedData_df = selectedCol_df.withColumn(\"data\", from_json(\"value\", schema))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "382e136b",
   "metadata": {},
   "source": [
    "parsedData_df.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0c4cf70",
   "metadata": {},
   "source": [
    "# เลือกคอลัมน์ที่ต้องการจาก DataFrame และแยก nested column ออกมา (ทำ unnest)\n",
    "\n",
    "unNested_df = parsedData_df.select(\"key\", \"data.*\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d649223a",
   "metadata": {},
   "source": [
    "unNested_df.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "47c36910",
   "metadata": {},
   "source": [
    "# แสดง schema เพื่อจะใช้ในการแปลงข้อมูลที่มีโครงสร้างเป็น JSON ใน column value (คนละตัวกับ value ก่อนหน้านี้) เป็น schema ที่กำหนดไว้\n",
    "# แสดง schema เพื่อจะใช้ในการแปลงข้อมูลที่มีโครงสร้างเป็น JSON ใน column date เป็น schema ที่กำหนดไว้\n",
    "# แสดง schema เพื่อจะใช้ในการแปลงข้อมูลที่มีโครงสร้างเป็น JSON ใน column coordiates เป็น schema ที่กำหนดไว้\n",
    "\n",
    "unNested_df.withColumn(\"data\", from_json(\"value\", schema))\\\n",
    ".withColumn(\"date\", from_json(\"date\", date_schema))\\\n",
    ".withColumn(\"coordinates\", from_json(\"coordinates\", coordinates_schema))\\\n",
    ".printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c55553f7",
   "metadata": {},
   "source": [
    "# แปลงข้อมูลที่มีโครงสร้างเป็น JSON ใน column value (คนละตัวกับ value ก่อนหน้านี้) เป็น schema ที่กำหนดไว้\n",
    "# แปลงข้อมูลที่มีโครงสร้างเป็น JSON ใน column date เป็น schema ที่กำหนดไว้\n",
    "# แปลงข้อมูลที่มีโครงสร้างเป็น JSON ใน column coordiates เป็น schema ที่กำหนดไว้\n",
    "# แล้วจึงทำ unnest ด้วย .select กับ col. data, date, coordinate\n",
    "\n",
    "extractedDateLatLong_df = unNested_df.withColumn(\"data\", from_json(\"value\", schema))\\\n",
    ".withColumn(\"date\", from_json(\"date\", date_schema))\\\n",
    ".withColumn(\"coordinates\", from_json(\"coordinates\", coordinates_schema))\\\n",
    ".select('key',\n",
    " 'locationId',\n",
    " 'location',\n",
    " 'parameter',\n",
    " 'value',\n",
    " 'date.*',\n",
    " 'unit',\n",
    " 'coordinates.*',\n",
    " 'country',\n",
    " 'city',\n",
    " 'isMobile',\n",
    " 'isAnalysis',\n",
    " 'entity',\n",
    " 'sensorType')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "036d8a66",
   "metadata": {},
   "source": [
    "extractedDateLatLong_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a4bcb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extractedDateLatLong_df.filter(sparkf.col('isMobile').isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bac2327",
   "metadata": {},
   "source": [
    "# 3. ได้รับ Result table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6e58596",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table_df = unboundInput_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "190857b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['key', 'value', 'topic', 'partition', 'offset', 'timestamp', 'timestampType']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# แสดงรายชื่อ column เป็น list\n",
    "\n",
    "result_table_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e24ee876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_table_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de33b473",
   "metadata": {},
   "source": [
    "# 4. ได้รับ Output (table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0b82ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/22 00:31:38 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1fea67e6-092c-48a7-bc68-9ff4ec3196dc. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "|key|value|topic|partition|offset|timestamp|timestampType|\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----+--------------------+-----------------+---------+------+--------------------+-------------+\n",
      "| key|               value|            topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----------------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|     0|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|     1|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|     2|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|     3|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|     4|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|     5|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|     6|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|     7|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|     8|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|     9|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|    10|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|    11|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|    12|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|    13|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|    14|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|    15|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|    16|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|    17|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|    18|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|    19|2023-06-22 00:31:...|            0|\n",
      "+----+--------------------+-----------------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/22 00:31:55 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 5257 milliseconds\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+----+--------------------+-----------------+---------+------+--------------------+-------------+\n",
      "| key|               value|            topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----------------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  1480|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  1481|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  1482|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  1483|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  1484|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  1485|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  1486|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  1487|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  1488|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  1489|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  1490|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  1491|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  1492|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  1493|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  1494|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  1495|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  1496|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  1497|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  1498|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  1499|2023-06-22 00:31:...|            0|\n",
      "+----+--------------------+-----------------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+----+--------------------+-----------------+---------+------+--------------------+-------------+\n",
      "| key|               value|            topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----------------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  3960|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  3961|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  3962|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  3963|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  3964|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  3965|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  3966|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  3967|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  3968|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  3969|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  3970|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  3971|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  3972|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  3973|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  3974|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  3975|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  3976|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  3977|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  3978|2023-06-22 00:31:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  3979|2023-06-22 00:31:...|            0|\n",
      "+----+--------------------+-----------------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+----+--------------------+-----------------+---------+------+--------------------+-------------+\n",
      "| key|               value|            topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----------------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  6173|2023-06-22 00:32:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  6174|2023-06-22 00:32:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  6175|2023-06-22 00:32:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  6176|2023-06-22 00:32:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  6177|2023-06-22 00:32:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  6178|2023-06-22 00:32:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  6179|2023-06-22 00:32:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  6180|2023-06-22 00:32:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  6181|2023-06-22 00:32:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  6182|2023-06-22 00:32:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  6183|2023-06-22 00:32:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  6184|2023-06-22 00:32:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  6185|2023-06-22 00:32:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  6186|2023-06-22 00:32:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  6187|2023-06-22 00:32:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  6188|2023-06-22 00:32:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  6189|2023-06-22 00:32:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  6190|2023-06-22 00:32:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  6191|2023-06-22 00:32:...|            0|\n",
      "|null|[7B 22 6C 6F 63 6...|quickstart-events|        0|  6192|2023-06-22 00:32:...|            0|\n",
      "+----+--------------------+-----------------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "\n",
    "output_df = result_table_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()\n",
    "\n",
    "output_df.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4bbb7e44",
   "metadata": {},
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# กำหนดฟังก์ชันสำหรับการตรวจสอบค่ามากที่สุดของฝุ่น ในแต่ละ trigger ที่ Spark ไป subscribe จาก Kafka มา (ทุกๆ 5 sec.)\n",
    "def max_value(df, epoch_id):\n",
    "    df = df.withColumn(\"value\", df[\"value\"].cast(\"float\"))\n",
    "    max_value_row = df.orderBy(df.value.desc()).first()\n",
    "    if max_value_row is not None and max_value_row.value > 60:\n",
    "        print(f\"ALERT! Batch: {epoch_id}, Max value: {max_value_row.value} exceeded 60 at date: {max_value_row.local}!\")\n",
    "    elif max_value_row is not None:\n",
    "        print(f\"Batch: {epoch_id}, Max value: {max_value_row.value} at date: {max_value_row.local}\")\n",
    "    else:\n",
    "        print(f\"Batch: {epoch_id}, No data\")\n",
    "\n",
    "output_df = result_table_df.writeStream \\\n",
    "    .foreachBatch(max_value) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()\n",
    "\n",
    "output_df.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5671aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d2904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
