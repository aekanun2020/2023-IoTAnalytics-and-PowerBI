{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc713a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-83d86c47-d314-484f-a0ab-0c1910447c56;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 641ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-83d86c47-d314-484f-a0ab-0c1910447c56\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/13ms)\n",
      "23/06/17 14:49:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# ปรับแต่งค่าการทำงานของ Spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"KafkaSubscribe\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"1000m\").\\\n",
    "        config(\"spark.executor.cores\", \"2\").\\\n",
    "        config(\"spark.cores.max\", \"6\").\\\n",
    "        config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0\").\\\n",
    "        getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fe8cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawMetadata_df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"10.128.0.12:9092\") \\\n",
    "  .option(\"subscribe\", \"quickstart-events\") \\\n",
    "  .option(\"group.id\", \"Aekanun-Spark-App\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0db7398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sparkf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b5b2796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawMetadata_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abd20d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# เลือกเฉพาะคอลัมน์ที่ต้องการ\n",
    "onlyMetadata_df = rawMetadata_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2db52707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "# Main schema\n",
    "schema = StructType([\n",
    "    StructField(\"locationId\", StringType()),\n",
    "    StructField(\"location\", StringType()),\n",
    "    StructField(\"parameter\", StringType()),\n",
    "    StructField(\"value\", StringType()),\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"unit\", StringType()),\n",
    "    StructField(\"coordinates\", StringType()),\n",
    "    StructField(\"country\", StringType()),\n",
    "    StructField(\"city\", StringType()),\n",
    "    StructField(\"isMobile\", StringType()),\n",
    "    StructField(\"isAnalysis\", StringType()),\n",
    "    StructField(\"entity\", StringType()),\n",
    "    StructField(\"sensorType\", StringType())\n",
    "])\n",
    "\n",
    "# Nested 'date' and 'coordinates' schema\n",
    "date_schema = StructType([\n",
    "    StructField(\"utc\", StringType()),\n",
    "    StructField(\"local\", StringType())\n",
    "])\n",
    "\n",
    "coordinates_schema = StructType([\n",
    "    StructField(\"latitude\", DoubleType()),\n",
    "    StructField(\"longitude\", DoubleType())\n",
    "])\n",
    "\n",
    "# Parse the JSON string column and convert it to a struct.\n",
    "parsedData_df = onlyMetadata_df.withColumn(\"data\", from_json(\"value\", schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dc40ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- locationId: string (nullable = true)\n",
      " |    |-- location: string (nullable = true)\n",
      " |    |-- parameter: string (nullable = true)\n",
      " |    |-- value: string (nullable = true)\n",
      " |    |-- date: string (nullable = true)\n",
      " |    |-- unit: string (nullable = true)\n",
      " |    |-- coordinates: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- isMobile: string (nullable = true)\n",
      " |    |-- isAnalysis: string (nullable = true)\n",
      " |    |-- entity: string (nullable = true)\n",
      " |    |-- sensorType: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsedData_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "811238cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the nested fields\n",
    "unNested_df = parsedData_df.select(\"key\", \"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "050d5abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- locationId: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- parameter: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- unit: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- isMobile: string (nullable = true)\n",
      " |-- isAnalysis: string (nullable = true)\n",
      " |-- entity: string (nullable = true)\n",
      " |-- sensorType: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unNested_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6e59755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- locationId: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- parameter: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- date: struct (nullable = true)\n",
      " |    |-- utc: string (nullable = true)\n",
      " |    |-- local: string (nullable = true)\n",
      " |-- unit: string (nullable = true)\n",
      " |-- coordinates: struct (nullable = true)\n",
      " |    |-- latitude: double (nullable = true)\n",
      " |    |-- longitude: double (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- isMobile: string (nullable = true)\n",
      " |-- isAnalysis: string (nullable = true)\n",
      " |-- entity: string (nullable = true)\n",
      " |-- sensorType: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- locationId: string (nullable = true)\n",
      " |    |-- location: string (nullable = true)\n",
      " |    |-- parameter: string (nullable = true)\n",
      " |    |-- value: string (nullable = true)\n",
      " |    |-- date: string (nullable = true)\n",
      " |    |-- unit: string (nullable = true)\n",
      " |    |-- coordinates: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- isMobile: string (nullable = true)\n",
      " |    |-- isAnalysis: string (nullable = true)\n",
      " |    |-- entity: string (nullable = true)\n",
      " |    |-- sensorType: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parse the JSON string column and convert it to a struct.\n",
    "unNested_df.withColumn(\"data\", from_json(\"value\", schema))\\\n",
    ".withColumn(\"date\", from_json(\"date\", date_schema))\\\n",
    ".withColumn(\"coordinates\", from_json(\"coordinates\", coordinates_schema))\\\n",
    ".printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bfd9d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractedDateLatLong_df = unNested_df.withColumn(\"data\", from_json(\"value\", schema))\\\n",
    ".withColumn(\"date\", from_json(\"date\", date_schema))\\\n",
    ".withColumn(\"coordinates\", from_json(\"coordinates\", coordinates_schema))\\\n",
    ".select('key',\n",
    " 'locationId',\n",
    " 'location',\n",
    " 'parameter',\n",
    " 'value',\n",
    " 'date.*',\n",
    " 'unit',\n",
    " 'coordinates.*',\n",
    " 'country',\n",
    " 'city',\n",
    " 'isMobile',\n",
    " 'isAnalysis',\n",
    " 'entity',\n",
    " 'sensorType')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "008354e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- locationId: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- parameter: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- utc: string (nullable = true)\n",
      " |-- local: string (nullable = true)\n",
      " |-- unit: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- isMobile: string (nullable = true)\n",
      " |-- isAnalysis: string (nullable = true)\n",
      " |-- entity: string (nullable = true)\n",
      " |-- sensorType: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extractedDateLatLong_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fad129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = extractedDateLatLong_df\\\n",
    ".withColumn('sourceUnixSTP',sparkf.unix_timestamp(sparkf.col('utc'), \"yyyy-MM-dd'T'HH:mm:ssXXX\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf8a0f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['key',\n",
       " 'locationId',\n",
       " 'location',\n",
       " 'parameter',\n",
       " 'value',\n",
       " 'utc',\n",
       " 'local',\n",
       " 'unit',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'country',\n",
       " 'city',\n",
       " 'isMobile',\n",
       " 'isAnalysis',\n",
       " 'entity',\n",
       " 'sensorType',\n",
       " 'sourceUnixSTP']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c098443c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- locationId: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- parameter: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- utc: string (nullable = true)\n",
      " |-- local: string (nullable = true)\n",
      " |-- unit: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- isMobile: string (nullable = true)\n",
      " |-- isAnalysis: string (nullable = true)\n",
      " |-- entity: string (nullable = true)\n",
      " |-- sensorType: string (nullable = true)\n",
      " |-- sourceUnixSTP: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92c3bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agg_df = final_df.groupBy('value').agg(sparkf.max(sparkf.col('value')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a079b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/17 14:49:36 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-fe8e477c-0dcc-40d1-adca-8ab2e2f19da3. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, No data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/17 14:49:46 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 6973 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1, Max value: 34.0 at date: 2022-01-28T10:00:00+07:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/17 14:49:53 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 5000 milliseconds, but spent 6330 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 2, Max value: 29.0 at date: 2022-01-29T17:00:00+07:00\n",
      "Batch: 3, Max value: 24.0 at date: 2022-01-31T01:00:00+07:00\n",
      "Batch: 4, Max value: 19.0 at date: 2022-02-02T01:00:00+07:00\n",
      "Batch: 5, Max value: 20.0 at date: 2022-02-03T19:00:00+07:00\n",
      "Batch: 6, Max value: 28.0 at date: 2022-02-05T09:00:00+07:00\n",
      "Batch: 7, Max value: 42.0 at date: 2022-02-06T23:00:00+07:00\n",
      "ALERT! Batch: 8, Max value: 63.0 exceeded 60 at date: 2022-02-07T22:00:00+07:00!\n",
      "ALERT! Batch: 9, Max value: 63.0 exceeded 60 at date: 2022-02-08T16:00:00+07:00!\n",
      "Batch: 10, Max value: 30.0 at date: 2022-02-10T12:00:00+07:00\n",
      "Batch: 11, Max value: 31.0 at date: 2022-02-12T17:00:00+07:00\n",
      "Batch: 12, Max value: 38.0 at date: 2022-02-13T03:00:00+07:00\n",
      "Batch: 13, Max value: 29.0 at date: 2022-02-15T13:00:00+07:00\n",
      "Batch: 14, Max value: 51.0 at date: 2022-02-17T00:00:00+07:00\n",
      "ALERT! Batch: 15, Max value: 61.0 exceeded 60 at date: 2022-02-17T10:00:00+07:00!\n",
      "Batch: 16, Max value: 25.0 at date: 2022-02-18T11:00:00+07:00\n",
      "Batch: 17, Max value: 14.0 at date: 2022-02-20T06:00:00+07:00\n",
      "Batch: 18, Max value: 30.0 at date: 2022-02-22T19:00:00+07:00\n",
      "Batch: 19, Max value: 40.0 at date: 2022-02-23T17:00:00+07:00\n",
      "Batch: 20, Max value: 44.0 at date: 2022-02-25T16:00:00+07:00\n",
      "Batch: 21, Max value: 52.0 at date: 2022-02-27T09:00:00+07:00\n",
      "Batch: 22, Max value: 58.0 at date: 2022-02-28T17:00:00+07:00\n",
      "Batch: 23, Max value: 41.0 at date: 2022-03-01T14:00:00+07:00\n",
      "Batch: 24, Max value: 25.0 at date: 2022-03-04T23:00:00+07:00\n",
      "Batch: 25, Max value: 28.0 at date: 2022-03-06T03:00:00+07:00\n",
      "Batch: 26, Max value: 25.0 at date: 2022-03-06T20:00:00+07:00\n",
      "Batch: 27, Max value: 58.0 at date: 2022-03-09T15:00:00+07:00\n",
      "ALERT! Batch: 28, Max value: 64.0 exceeded 60 at date: 2022-03-09T20:00:00+07:00!\n",
      "Batch: 29, Max value: 30.0 at date: 2022-03-11T01:00:00+07:00\n",
      "Batch: 30, Max value: 31.0 at date: 2022-03-13T02:00:00+07:00\n",
      "Batch: 31, Max value: 34.0 at date: 2022-03-14T11:00:00+07:00\n",
      "Batch: 32, Max value: 20.0 at date: 2022-03-15T10:00:00+07:00\n",
      "Batch: 33, Max value: 18.0 at date: 2022-03-17T14:00:00+07:00\n",
      "Batch: 34, Max value: 33.0 at date: 2022-03-19T17:00:00+07:00\n",
      "Batch: 35, Max value: 33.0 at date: 2022-03-19T21:00:00+07:00\n",
      "Batch: 36, Max value: 19.0 at date: 2022-03-21T14:00:00+07:00\n",
      "Batch: 37, Max value: 18.0 at date: 2022-03-25T13:00:00+07:00\n",
      "Batch: 38, Max value: 16.0 at date: 2022-03-26T22:00:00+07:00\n",
      "Batch: 39, Max value: 21.0 at date: 2022-03-29T08:00:00+07:00\n",
      "Batch: 40, Max value: 35.0 at date: 2022-04-01T17:00:00+07:00\n",
      "Batch: 41, Max value: 32.0 at date: 2022-04-01T21:00:00+07:00\n",
      "Batch: 42, Max value: 24.0 at date: 2022-04-04T13:00:00+07:00\n",
      "Batch: 43, Max value: 36.0 at date: 2022-04-05T23:00:00+07:00\n",
      "Batch: 44, Max value: 56.0 at date: 2022-04-06T15:00:00+07:00\n",
      "Batch: 45, Max value: 56.0 at date: 2022-04-08T16:00:00+07:00\n",
      "ALERT! Batch: 46, Max value: 73.0 exceeded 60 at date: 2022-04-10T03:00:00+07:00!\n",
      "ALERT! Batch: 47, Max value: 73.0 exceeded 60 at date: 2022-04-10T04:00:00+07:00!\n",
      "Batch: 48, Max value: 50.0 at date: 2022-04-12T10:00:00+07:00\n",
      "Batch: 49, Max value: 49.0 at date: 2022-04-13T00:00:00+07:00\n",
      "Batch: 50, Max value: 35.0 at date: 2022-04-15T08:00:00+07:00\n",
      "Batch: 51, Max value: 34.0 at date: 2022-04-15T19:00:00+07:00\n",
      "Batch: 52, Max value: 33.0 at date: 2022-04-18T10:00:00+07:00\n",
      "Batch: 53, Max value: 35.0 at date: 2022-04-18T16:00:00+07:00\n",
      "Batch: 54, Max value: 52.0 at date: 2022-04-20T15:00:00+07:00\n",
      "Batch: 55, Max value: 39.0 at date: 2022-04-21T09:00:00+07:00\n",
      "Batch: 56, Max value: 21.0 at date: 2022-04-22T19:00:00+07:00\n",
      "Batch: 57, Max value: 15.0 at date: 2022-04-24T17:00:00+07:00\n",
      "Batch: 58, Max value: 15.0 at date: 2022-04-25T14:00:00+07:00\n",
      "Batch: 59, Max value: 24.0 at date: 2022-04-28T08:00:00+07:00\n",
      "Batch: 60, Max value: 41.0 at date: 2022-04-30T19:00:00+07:00\n",
      "Batch: 61, Max value: 42.0 at date: 2022-04-30T22:00:00+07:00\n",
      "Batch: 62, Max value: 26.0 at date: 2022-05-02T06:00:00+07:00\n",
      "Batch: 63, Max value: 26.0 at date: 2022-05-05T04:00:00+07:00\n",
      "Batch: 64, Max value: 36.0 at date: 2022-05-06T13:00:00+07:00\n",
      "Batch: 65, Max value: 37.0 at date: 2022-05-06T16:00:00+07:00\n",
      "Batch: 66, Max value: 28.0 at date: 2022-05-07T23:00:00+07:00\n",
      "Batch: 67, Max value: 27.0 at date: 2022-05-09T13:00:00+07:00\n",
      "Batch: 68, Max value: 15.0 at date: 2022-05-11T00:00:00+07:00\n",
      "Batch: 69, Max value: 16.0 at date: 2022-05-12T17:00:00+07:00\n",
      "Batch: 70, Max value: 22.0 at date: 2022-05-14T09:00:00+07:00\n",
      "Batch: 71, Max value: 23.0 at date: 2022-05-15T18:00:00+07:00\n",
      "Batch: 72, Max value: 21.0 at date: 2022-05-16T08:00:00+07:00\n",
      "Batch: 73, Max value: 23.0 at date: 2022-05-18T13:00:00+07:00\n",
      "Batch: 74, Max value: 21.0 at date: 2022-05-19T08:00:00+07:00\n",
      "Batch: 75, Max value: 9.0 at date: 2022-05-21T23:00:00+07:00\n",
      "Batch: 76, Max value: 13.0 at date: 2022-05-24T06:00:00+07:00\n",
      "Batch: 77, Max value: 17.0 at date: 2022-05-25T19:00:00+07:00\n",
      "Batch: 78, Max value: 18.0 at date: 2022-05-27T00:00:00+07:00\n",
      "Batch: 79, Max value: 29.0 at date: 2022-05-29T18:00:00+07:00\n",
      "Batch: 80, Max value: 28.0 at date: 2022-05-29T23:00:00+07:00\n",
      "Batch: 81, Max value: 27.0 at date: 2022-06-01T09:00:00+07:00\n",
      "Batch: 82, Max value: 28.0 at date: 2022-06-01T18:00:00+07:00\n",
      "Batch: 83, Max value: 18.0 at date: 2022-06-03T03:00:00+07:00\n",
      "Batch: 84, Max value: 12.0 at date: 2022-06-04T23:00:00+07:00\n",
      "Batch: 85, Max value: 17.0 at date: 2022-06-07T07:00:00+07:00\n",
      "Batch: 86, Max value: 19.0 at date: 2022-06-07T09:00:00+07:00\n",
      "Batch: 87, Max value: 16.0 at date: 2022-06-08T17:00:00+07:00\n",
      "Batch: 88, Max value: 16.0 at date: 2022-06-11T04:00:00+07:00\n",
      "Batch: 89, Max value: 19.0 at date: 2022-06-12T00:00:00+07:00\n",
      "Batch: 90, Max value: 21.0 at date: 2022-06-14T04:00:00+07:00\n",
      "Batch: 91, Max value: 21.0 at date: 2022-06-14T08:00:00+07:00\n",
      "Batch: 92, Max value: 18.0 at date: 2022-06-15T18:00:00+07:00\n",
      "Batch: 93, Max value: 20.0 at date: 2022-06-17T19:00:00+07:00\n",
      "Batch: 94, Max value: 19.0 at date: 2022-06-21T06:00:00+07:00\n",
      "Batch: 95, Max value: 19.0 at date: 2022-06-22T15:00:00+07:00\n",
      "Batch: 96, Max value: 19.0 at date: 2022-06-22T20:00:00+07:00\n",
      "Batch: 97, Max value: 19.0 at date: 2022-06-25T13:00:00+07:00\n",
      "Batch: 98, Max value: 20.0 at date: 2022-06-25T15:00:00+07:00\n",
      "Batch: 99, Max value: 23.0 at date: 2022-06-29T08:00:00+07:00\n",
      "Batch: 100, Max value: 23.0 at date: 2022-06-29T10:00:00+07:00\n",
      "Batch: 101, Max value: 17.0 at date: 2022-07-01T09:00:00+07:00\n",
      "Batch: 102, Max value: 16.0 at date: 2022-07-03T07:00:00+07:00\n",
      "Batch: 103, Max value: 15.0 at date: 2022-07-03T14:00:00+07:00\n",
      "Batch: 104, Max value: 15.0 at date: 2022-07-06T08:00:00+07:00\n",
      "Batch: 105, Max value: 19.0 at date: 2022-07-07T05:00:00+07:00\n",
      "Batch: 106, Max value: 15.0 at date: 2022-07-07T19:00:00+07:00\n",
      "Batch: 107, Max value: 17.0 at date: 2022-07-09T11:00:00+07:00\n",
      "Batch: 108, Max value: 15.0 at date: 2022-07-11T21:00:00+07:00\n",
      "Batch: 109, Max value: 21.0 at date: 2022-07-13T07:00:00+07:00\n",
      "Batch: 110, Max value: 23.0 at date: 2022-07-14T11:00:00+07:00\n",
      "Batch: 111, Max value: 25.0 at date: 2022-07-15T11:00:00+07:00\n",
      "Batch: 112, Max value: 24.0 at date: 2022-07-16T02:00:00+07:00\n",
      "Batch: 113, Max value: 17.0 at date: 2022-07-17T11:00:00+07:00\n",
      "Batch: 114, Max value: 16.0 at date: 2022-07-20T01:00:00+07:00\n",
      "Batch: 115, Max value: 18.0 at date: 2022-07-27T08:00:00+07:00\n",
      "Batch: 116, Max value: 18.0 at date: 2022-07-28T05:00:00+07:00\n",
      "Batch: 117, Max value: 16.0 at date: 2022-07-30T22:00:00+07:00\n",
      "Batch: 118, Max value: 18.0 at date: 2022-08-01T12:00:00+07:00\n",
      "Batch: 119, Max value: 20.0 at date: 2022-08-01T22:00:00+07:00\n",
      "Batch: 120, Max value: 15.0 at date: 2022-08-03T03:00:00+07:00\n",
      "Batch: 121, Max value: 17.0 at date: 2022-08-04T22:00:00+07:00\n",
      "Batch: 122, Max value: 21.0 at date: 2022-08-06T05:00:00+07:00\n",
      "Batch: 123, Max value: 13.0 at date: 2022-08-07T03:00:00+07:00\n",
      "Batch: 124, Max value: 17.0 at date: 2022-08-09T21:00:00+07:00\n",
      "Batch: 125, Max value: 19.0 at date: 2022-08-10T07:00:00+07:00\n",
      "Batch: 126, Max value: 17.0 at date: 2022-08-12T12:00:00+07:00\n",
      "Batch: 127, Max value: 17.0 at date: 2022-08-12T16:00:00+07:00\n",
      "Batch: 128, Max value: 16.0 at date: 2022-08-14T20:00:00+07:00\n",
      "Batch: 129, Max value: 13.0 at date: 2022-08-15T11:00:00+07:00\n",
      "Batch: 130, Max value: 24.0 at date: 2022-08-17T15:00:00+07:00\n",
      "Batch: 131, Max value: 22.0 at date: 2022-08-18T05:00:00+07:00\n",
      "Batch: 132, Max value: 18.0 at date: 2022-08-19T15:00:00+07:00\n",
      "Batch: 133, Max value: 21.0 at date: 2022-08-21T10:00:00+07:00\n",
      "Batch: 134, Max value: 20.0 at date: 2022-08-23T18:00:00+07:00\n",
      "Batch: 135, Max value: 21.0 at date: 2022-08-23T21:00:00+07:00\n",
      "Batch: 136, Max value: 24.0 at date: 2022-08-28T04:00:00+07:00\n",
      "Batch: 137, Max value: 24.0 at date: 2022-08-30T14:00:00+07:00\n",
      "Batch: 138, Max value: 35.0 at date: 2022-08-31T09:00:00+07:00\n",
      "Batch: 139, Max value: 32.0 at date: 2022-09-02T05:00:00+07:00\n",
      "Batch: 140, Max value: 32.0 at date: 2022-09-02T17:00:00+07:00\n",
      "Batch: 141, Max value: 23.0 at date: 2022-09-04T17:00:00+07:00\n",
      "Batch: 142, Max value: 17.0 at date: 2022-09-05T11:00:00+07:00\n",
      "Batch: 143, Max value: 15.0 at date: 2022-09-06T19:00:00+07:00\n",
      "Batch: 144, Max value: 21.0 at date: 2022-09-09T11:00:00+07:00\n",
      "Batch: 145, Max value: 22.0 at date: 2022-09-09T14:00:00+07:00\n",
      "Batch: 146, Max value: 13.0 at date: 2022-09-10T22:00:00+07:00\n",
      "Batch: 147, Max value: 25.0 at date: 2022-09-13T15:00:00+07:00\n",
      "Batch: 148, Max value: 26.0 at date: 2022-09-13T16:00:00+07:00\n",
      "Batch: 149, Max value: 19.0 at date: 2022-09-15T01:00:00+07:00\n",
      "Batch: 150, Max value: 15.0 at date: 2022-09-17T08:00:00+07:00\n",
      "Batch: 151, Max value: 17.0 at date: 2022-09-18T10:00:00+07:00\n",
      "Batch: 152, Max value: 14.0 at date: 2022-09-20T02:00:00+07:00\n",
      "Batch: 153, Max value: 15.0 at date: 2022-09-20T22:00:00+07:00\n",
      "Batch: 154, Max value: 30.0 at date: 2022-09-23T09:00:00+07:00\n",
      "Batch: 155, Max value: 32.0 at date: 2022-09-23T23:00:00+07:00\n",
      "Batch: 156, Max value: 26.0 at date: 2022-09-26T19:00:00+07:00\n",
      "Batch: 157, Max value: 26.0 at date: 2022-09-27T16:00:00+07:00\n",
      "Batch: 158, Max value: 16.0 at date: 2022-09-28T17:00:00+07:00\n",
      "Batch: 159, Max value: 16.0 at date: 2022-09-30T02:00:00+07:00\n",
      "Batch: 160, Max value: 20.0 at date: 2022-10-02T20:00:00+07:00\n",
      "Batch: 161, Max value: 25.0 at date: 2022-10-03T11:00:00+07:00\n",
      "Batch: 162, Max value: 28.0 at date: 2022-10-05T11:00:00+07:00\n",
      "Batch: 163, Max value: 32.0 at date: 2022-10-06T13:00:00+07:00\n",
      "Batch: 164, Max value: 30.0 at date: 2022-10-12T10:00:00+07:00\n",
      "Batch: 165, Max value: 35.0 at date: 2022-10-13T02:00:00+07:00\n",
      "Batch: 166, Max value: 40.0 at date: 2022-10-15T05:00:00+07:00\n",
      "Batch: 167, Max value: 43.0 at date: 2022-10-15T14:00:00+07:00\n",
      "Batch: 168, Max value: 39.0 at date: 2022-10-18T00:00:00+07:00\n",
      "Batch: 169, Max value: 46.0 at date: 2022-10-18T09:00:00+07:00\n",
      "Batch: 170, Max value: 53.0 at date: 2022-10-20T07:00:00+07:00\n",
      "Batch: 171, Max value: 49.0 at date: 2022-10-20T19:00:00+07:00\n",
      "Batch: 172, Max value: 40.0 at date: 2022-10-22T11:00:00+07:00\n",
      "Batch: 173, Max value: 51.0 at date: 2022-10-25T08:00:00+07:00\n",
      "Batch: 174, Max value: 49.0 at date: 2022-10-25T23:00:00+07:00\n",
      "Batch: 175, Max value: 29.0 at date: 2022-10-27T08:00:00+07:00\n",
      "Batch: 176, Max value: 26.0 at date: 2022-10-30T00:00:00+07:00\n",
      "Batch: 177, Max value: 37.0 at date: 2022-10-31T11:00:00+07:00\n",
      "Batch: 178, Max value: 48.0 at date: 2022-11-01T10:00:00+07:00\n",
      "Batch: 179, Max value: 41.0 at date: 2022-11-01T21:00:00+07:00\n",
      "Batch: 180, Max value: 38.0 at date: 2022-11-03T17:00:00+07:00\n",
      "Batch: 181, Max value: 54.0 at date: 2022-11-06T00:00:00+07:00\n",
      "Batch: 182, Max value: 53.0 at date: 2022-11-06T01:00:00+07:00\n",
      "Batch: 183, Max value: 55.0 at date: 2022-11-08T19:00:00+07:00\n",
      "Batch: 184, Max value: 48.0 at date: 2022-11-10T03:00:00+07:00\n",
      "Batch: 185, Max value: 37.0 at date: 2022-11-12T02:00:00+07:00\n",
      "Batch: 186, Max value: 36.0 at date: 2022-11-12T11:00:00+07:00\n",
      "Batch: 187, Max value: 32.0 at date: 2022-11-13T22:00:00+07:00\n",
      "Batch: 188, Max value: 30.0 at date: 2022-11-16T13:00:00+07:00\n",
      "Batch: 189, Max value: 39.0 at date: 2022-11-17T18:00:00+07:00\n",
      "Batch: 190, Max value: 37.0 at date: 2022-11-18T00:00:00+07:00\n",
      "Batch: 191, Max value: 37.0 at date: 2022-11-19T11:00:00+07:00\n",
      "Batch: 192, Max value: 28.0 at date: 2022-11-22T03:00:00+07:00\n",
      "Batch: 193, Max value: 41.0 at date: 2022-11-23T07:00:00+07:00\n",
      "Batch: 194, Max value: 31.0 at date: 2022-11-24T13:00:00+07:00\n",
      "Batch: 195, Max value: 23.0 at date: 2022-11-26T11:00:00+07:00\n",
      "Batch: 196, Max value: 26.0 at date: 2022-11-28T00:00:00+07:00\n",
      "Batch: 197, Max value: 41.0 at date: 2022-11-30T03:00:00+07:00\n",
      "Batch: 198, Max value: 53.0 at date: 2022-11-30T17:00:00+07:00\n",
      "Batch: 199, Max value: 30.0 at date: 2022-12-02T17:00:00+07:00\n",
      "Batch: 200, Max value: 36.0 at date: 2022-12-03T06:00:00+07:00\n",
      "Batch: 201, Max value: 37.0 at date: 2022-12-05T03:00:00+07:00\n",
      "Batch: 202, Max value: 37.0 at date: 2022-12-05T18:00:00+07:00\n",
      "Batch: 203, Max value: 32.0 at date: 2022-12-08T06:00:00+07:00\n",
      "Batch: 204, Max value: 43.0 at date: 2022-12-08T13:00:00+07:00\n",
      "Batch: 205, Max value: 49.0 at date: 2022-12-11T07:00:00+07:00\n",
      "Batch: 206, Max value: 49.0 at date: 2022-12-11T08:00:00+07:00\n",
      "Batch: 207, Max value: 43.0 at date: 2022-12-13T01:00:00+07:00\n",
      "Batch: 208, Max value: 52.0 at date: 2022-12-14T12:00:00+07:00\n",
      "Batch: 209, Max value: 54.0 at date: 2022-12-16T19:00:00+07:00\n",
      "Batch: 210, Max value: 55.0 at date: 2022-12-17T03:00:00+07:00\n",
      "Batch: 211, Max value: 38.0 at date: 2022-12-18T06:00:00+07:00\n",
      "Batch: 212, Max value: 46.0 at date: 2022-12-20T22:00:00+07:00\n",
      "Batch: 213, Max value: 55.0 at date: 2022-12-21T21:00:00+07:00\n",
      "ALERT! Batch: 214, Max value: 62.0 exceeded 60 at date: 2022-12-23T07:00:00+07:00!\n",
      "Batch: 215, Max value: 54.0 at date: 2022-12-25T00:00:00+07:00\n",
      "Batch: 216, Max value: 39.0 at date: 2022-12-26T12:00:00+07:00\n",
      "Batch: 217, Max value: 48.0 at date: 2022-12-28T11:00:00+07:00\n",
      "Batch: 218, Max value: 48.0 at date: 2022-12-29T10:00:00+07:00\n",
      "Batch: 219, Max value: 37.0 at date: 2022-12-31T14:00:00+07:00\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def max_value(df, epoch_id):\n",
    "    df = df.withColumn(\"value\", df[\"value\"].cast(\"float\"))\n",
    "    max_value_row = df.orderBy(df.value.desc()).first()\n",
    "    if max_value_row is not None and max_value_row.value > 60:\n",
    "        print(f\"ALERT! Batch: {epoch_id}, Max value: {max_value_row.value} exceeded 60 at date: {max_value_row.local}!\")\n",
    "    elif max_value_row is not None:\n",
    "        print(f\"Batch: {epoch_id}, Max value: {max_value_row.value} at date: {max_value_row.local}\")\n",
    "    else:\n",
    "        print(f\"Batch: {epoch_id}, No data\")\n",
    "\n",
    "query = final_df.writeStream \\\n",
    "    .foreachBatch(max_value) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f01e5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7cd926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
